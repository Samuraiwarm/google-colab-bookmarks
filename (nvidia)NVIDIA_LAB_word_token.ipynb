{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(nvidia)NVIDIA_LAB_word_token.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samuraiwarm/google-colab-bookmarks/blob/master/(nvidia)NVIDIA_LAB_word_token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hctJ61HmCjJY",
        "colab_type": "code",
        "outputId": "51d7a1ca-f3ca-4733-a8da-8dd2ecdcfc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.0.0`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LPt5Cffn2Z",
        "colab_type": "text"
      },
      "source": [
        "# check TF version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB076rQ_C5Bh",
        "colab_type": "code",
        "outputId": "54eca3eb-1509-4485-e869-bbc7020c6233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKBssDgiECzJ",
        "colab_type": "text"
      },
      "source": [
        "## Setting up to use the gpu  \n",
        "\n",
        "Before we start, we need to change the environment of Colab to use GPU. Do so by:\n",
        "\n",
        "Runtime -> Change runtime type -> Hardware accelerator -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teiwAR57ENeK",
        "colab_type": "code",
        "outputId": "529254f1-f78f-43e1-bfc4-110b1ecc1d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "## check gpu\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Sep 10 13:49:08 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    24W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhiDwZjOJdfW",
        "colab_type": "text"
      },
      "source": [
        "# pythaiNLP (mm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVouG8DgHoS5",
        "colab_type": "code",
        "outputId": "1c52d870-9761-4dc8-c085-8c9285ed2c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!pip install pythainlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pythainlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/94/1ca5c23bfdbc0f27fa26e5eeda47d8ff422cbbd3f38c0b8a160fa17a2583/pythainlp-2.0.7-py3-none-any.whl (11.0MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0MB 6.4MB/s \n",
            "\u001b[?25hCollecting tinydb (from pythainlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/f9/0e871cbf0da678cf1780609dc6aef26a5ed544c86733fc1ceaf134fce52c/tinydb-3.13.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pythainlp) (4.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2018.9)\n",
            "Collecting marisa-trie (from pythainlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.2 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (3.2.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pythainlp) (0.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: six in /tensorflow-2.0.0-rc0/python3.6 (from nltk>=3.2.2->pythainlp) (1.12.0)\n",
            "Building wheels for collected packages: marisa-trie\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp36-cp36m-linux_x86_64.whl size=861328 sha256=bcf39420f7e7b0e67f810849a6653979c5067f70f687cdb1c18e966a03f35ae1\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "Successfully built marisa-trie\n",
            "Installing collected packages: tinydb, marisa-trie, pythainlp\n",
            "Successfully installed marisa-trie-0.7.5 pythainlp-2.0.7 tinydb-3.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26EtD9EzJpUs",
        "colab_type": "code",
        "outputId": "8aeacbb2-4ad7-4de7-dac8-ee1050e0556f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "text = 'ผมรักคุณนะครับโอเคบ่พวกเราเป็นคนไทยรักภาษาไทยภาษาบ้านเกิด'\n",
        "token_01 = word_tokenize(text,engine='newmm')\n",
        "print(token_01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ผม', 'รัก', 'คุณ', 'นะ', 'ครับ', 'โอเค', 'บ่', 'พวกเรา', 'เป็น', 'คนไทย', 'รัก', 'ภาษาไทย', 'ภาษา', 'บ้านเกิด']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-xouJqHUHLA",
        "colab_type": "code",
        "outputId": "f8bc74cb-ee82-46a6-b661-e3dc678da4fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!ls /usr/local/lib/python3.6/dist-packages/pythainlp/corpus/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "common.py\t       thailand_provinces_th.txt\n",
            "conceptnet.py\t       tha-wn.db\n",
            "corpus_license.md      tnc_freq.txt\n",
            "countries_th.txt       tnc.py\n",
            "__init__.py\t       ttc_freq.txt\n",
            "negations_th.txt       ttc.py\n",
            "orchid_pos_th.json     ud_thai_pud_pt_tagger.dill\n",
            "orchid_pt_tagger.dill  ud_thai_pud_unigram_tagger.dill\n",
            "__pycache__\t       wordnet.py\n",
            "stopwords_th.txt       words_th_frozen_201810.txt\n",
            "syllables_th.txt       words_th.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFg2p6dGHfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /usr/local/lib/python3.6/dist-packages/pythainlp/corpus/words_th.txt ./\n",
        "!mv words_th.txt new_word_th.txt\n",
        "!echo \"สามย่านมิตรทาวน์\" >> new_word_th.txt\n",
        "!cp ./new_word_th.txt /usr/local/lib/python3.6/dist-packages/pythainlp/corpus/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itqRjuXzJ9-M",
        "colab_type": "code",
        "outputId": "3384fedd-a109-4d4a-a7ec-8f5cb1f4ade9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!ls /usr/local/lib/python3.6/dist-packages/pythainlp/corpus/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "common.py\t    orchid_pt_tagger.dill      ttc_freq.txt\n",
            "conceptnet.py\t    __pycache__\t\t       ttc.py\n",
            "corpus_license.md   stopwords_th.txt\t       ud_thai_pud_pt_tagger.dill\n",
            "countries_th.txt    syllables_th.txt\t       ud_thai_pud_unigram_tagger.dill\n",
            "__init__.py\t    thailand_provinces_th.txt  wordnet.py\n",
            "negations_th.txt    tha-wn.db\t\t       words_th_frozen_201810.txt\n",
            "new_word_th.txt     tnc_freq.txt\t       words_th.txt\n",
            "orchid_pos_th.json  tnc.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDBm3SXFF2H2",
        "colab_type": "code",
        "outputId": "ecc1a084-56a8-44b7-e274-173cae4f0bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "text = 'นัดกินกันตอนไหนก็ได้ที่สามย่านมิตรทาวน์'\n",
        "token_error = word_tokenize(text,engine='newmm')\n",
        "print(token_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่าน', 'มิตร', 'ทาวน์']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E0Of3E5P9_0",
        "colab_type": "code",
        "outputId": "a8ef10e5-7dca-4f29-dea9-fce38b59be5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pythainlp.corpus import get_corpus\n",
        "from marisa_trie import Trie\n",
        "\n",
        "text='นัดกินกันตอนไหนก็ได้ที่สามย่านมิตรทาวน์'\n",
        "new_true_tokenizer = Trie(get_corpus('new_word_th.txt'))\n",
        "\n",
        "tokens = word_tokenize(text, custom_dict=new_true_tokenizer, engine='newmm')\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่านมิตรทาวน์']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ytyWah2pqxI",
        "colab_type": "text"
      },
      "source": [
        "# mount your google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGL_Pe5VIO8p",
        "colab_type": "text"
      },
      "source": [
        "Download these files to your google drive\n",
        "\n",
        "[corpus](https://drive.google.com/file/d/12GSSZ90ZsyR-qN28rCEDg-EPOFyWPtiA/view?usp=sharing) \n",
        "\n",
        "[Model_weights](https://drive.google.com/file/d/1vOmrOMylO0I7V1gcla0czukWD7o-WduF/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8nPVt1CNTzL",
        "colab_type": "code",
        "outputId": "2f2e80a4-9a3b-458e-c1b8-39e88c109df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xID-GrB4IuxA",
        "colab_type": "code",
        "outputId": "90d340b4-96d7-4660-dfac-d49b398f02e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mkdir /content/corpora\n",
        "!unzip gdrive/My\\ Drive/BEST.zip -d /content/corpora\n",
        "!unzip gdrive/My\\ Drive/model_weight_token.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpora/\n",
            "corpora/mnist_data/\n",
            "corpora/mnist_data/t10k-images-idx3-ubyte.gz\n",
            "corpora/mnist_data/train-images-idx3-ubyte.gz\n",
            "corpora/mnist_data/.ipynb_checkpoints/\n",
            "corpora/mnist_data/vis_utils.py\n",
            "corpora/mnist_data/__init__.py\n",
            "corpora/mnist_data/load_mnist.py\n",
            "corpora/mnist_data/train-labels-idx1-ubyte.gz\n",
            "corpora/mnist_data/t10k-labels-idx1-ubyte.gz\n",
            "corpora/BEST/\n",
            "corpora/BEST/test/\n",
            "corpora/BEST/test/df_best_article_test.csv\n",
            "corpora/BEST/test/df_best_encyclopedia_test.csv\n",
            "corpora/BEST/test/df_best_novel_test.csv\n",
            "corpora/BEST/test/df_best_news_test.csv\n",
            "corpora/BEST/train/\n",
            "corpora/BEST/train/df_best_encyclopedia_train.csv\n",
            "corpora/BEST/train/df_best_article_train.csv\n",
            "corpora/BEST/train/df_best_news_train.csv\n",
            "corpora/BEST/train/df_best_novel_train.csv\n",
            "corpora/BEST/val/\n",
            "corpora/BEST/val/df_best_encyclopedia_val.csv\n",
            "corpora/BEST/val/df_best_news_val.csv\n",
            "corpora/BEST/val/df_best_article_val.csv\n",
            "corpora/BEST/val/df_best_novel_val.csv\n",
            "corpora/.ipynb_checkpoints/\n",
            "corpora/.ipynb_checkpoints/Word_Tokenizer.new-checkpoint.ipynb\n",
            "corpora/.ipynb_checkpoints/BackProp-checkpoint.ipynb\n",
            "corpora/.ipynb_checkpoints/Word_Tokenizer_backup-checkpoint.ipynb\n",
            "corpora/.ipynb_checkpoints/char2vec-checkpoint.ipynb\n",
            "corpora/.ipynb_checkpoints/Word_Tokenizer-checkpoint.ipynb\n",
            "corpora/cattern/\n",
            "corpora/cattern/gradient_check.py\n",
            "corpora/cattern/.ipynb_checkpoints/\n",
            "corpora/cattern/__init__.py\n",
            "corpora/cattern/data_utils.py\n",
            "corpora/wiki/\n",
            "corpora/wiki/thwiki_chk.txt\n",
            "Archive:  gdrive/My Drive/model_weight_token.zip\n",
            "   creating: model_weight_token/\n",
            "  inflating: model_weight_token/model_weight_feedforward_nn.h5  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/model_weight_token/\n",
            "  inflating: __MACOSX/model_weight_token/._model_weight_feedforward_nn.h5  \n",
            "  inflating: model_weight_token/model_weight_nn_dropout.h5  \n",
            "  inflating: __MACOSX/model_weight_token/._model_weight_nn_dropout.h5  \n",
            "  inflating: model_weight_token/model_weight_gru.h5  \n",
            "  inflating: __MACOSX/model_weight_token/._model_weight_gru.h5  \n",
            "  inflating: model_weight_token/model_weight_conv_nn.h5  \n",
            "  inflating: __MACOSX/model_weight_token/._model_weight_conv_nn.h5  \n",
            "--2019-09-10 13:50:18--  https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548640 (1.5M) [text/plain]\n",
            "Saving to: ‘words_th.txt’\n",
            "\n",
            "words_th.txt        100%[===================>]   1.48M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-09-10 13:50:19 (44.8 MB/s) - ‘words_th.txt’ saved [1548640/1548640]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMIslvQbbvHf",
        "colab_type": "code",
        "outputId": "5c822a1b-8abf-4214-ab99-3156a84cac10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpora  gdrive  __MACOSX  model_weight_token  sample_data  words_th.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aygu3JGqJBkP",
        "colab_type": "code",
        "outputId": "d8a4ba19-efc6-4973-b13a-db20b4148a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls corpora/BEST/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test  train  val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqtPrUuINvK",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNLQWSLBGNVs",
        "colab_type": "text"
      },
      "source": [
        "## Word Tokenizer exercise##\n",
        "\n",
        "In this exercise, you are going to build a set of deep learning models on a (sort of) real world task using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
        "\n",
        "To complete this exercise, you will need to build deep learning models for word tokenization in Thai (แบ่งเว้นวรรคภาษาไทย) using NECTEC's BEST corpus. You will build one model for each of the following type:\n",
        "- Fully Connected (Feedforward) Neural Network\n",
        "- One-Dimentional Convolution Neural Network (1D-CNN)\n",
        "- Recurrent Neural Network with Gated Recurrent Unit (GRU)\n",
        "\n",
        "and one more model of your choice to achieve the highest score possible.\n",
        "\n",
        "We provide the code for data cleaning and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
        "\n",
        "This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n",
        "\n",
        "As a reminder,\n",
        "\n",
        "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K5kBrJHQkRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run setup code\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDJ3GfXEPPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a character map\n",
        "CHARS = [\n",
        "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
        "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
        "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
        "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
        "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
        "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
        "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
        "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
        "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
        "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
        "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
        "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
        "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
        "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
        "]\n",
        "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5nfX2BTGiP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_n_gram_df(df, n_pad):\n",
        "  \"\"\"\n",
        "  Given an input dataframe, create a feature dataframe of shifted characters\n",
        "  Input:\n",
        "  df: timeseries of size (N)\n",
        "  n_pad: the number of context. For a given character at position [idx],\n",
        "    character at position [idx-n_pad/2 : idx+n_pad/2] will be used \n",
        "    as features for that character.\n",
        "  \n",
        "  Output:\n",
        "  dataframe of size (N * n_pad) which each row contains the character, \n",
        "    n_pad_2 characters to the left, and n_pad_2 characters to the right\n",
        "    of that character.\n",
        "  \"\"\"\n",
        "  n_pad_2 = int((n_pad - 1)/2)\n",
        "  for i in range(n_pad_2):\n",
        "      df['char-{}'.format(i+1)] = df['char'].shift(i + 1)\n",
        "      df['char{}'.format(i+1)] = df['char'].shift(-i - 1)\n",
        "  return df[n_pad_2: -n_pad_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u_vSQFiG3U2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_feature(best_processed_path, option='train'):\n",
        "  \"\"\"\n",
        "  Transform the path to a directory containing processed files \n",
        "  into a feature matrix and output array\n",
        "  Input:\n",
        "  best_processed_path: str, path to a processed version of the BEST dataset\n",
        "  option: str, 'train' or 'test'\n",
        "  \"\"\"\n",
        "  # we use padding equals 21 here to consider 10 characters to the left\n",
        "  # and 10 characters to the right as features for the character in the middle\n",
        "  n_pad = 21\n",
        "  n_pad_2 = int((n_pad - 1)/2)\n",
        "  pad = [{'char': ' ', 'target': True}]\n",
        "  df_pad = pd.DataFrame(pad * n_pad_2)\n",
        "\n",
        "  df = []\n",
        "  # article types in BEST corpus\n",
        "  article_types = ['article', 'encyclopedia', 'news', 'novel']\n",
        "  for article_type in article_types:\n",
        "      df.append(pd.read_csv(os.path.join(best_processed_path, option, 'df_best_{}_{}.csv'.format(article_type, option))))\n",
        "  \n",
        "  df = pd.concat(df)\n",
        "  # pad with empty string feature\n",
        "  df = pd.concat((df_pad, df, df_pad))\n",
        "\n",
        "  # map characters to numbers, use 'other' if not in the predefined character set.\n",
        "  df['char'] = df['char'].map(lambda x: CHARS_MAP.get(x, 80))\n",
        "\n",
        "  # Use nearby characters as features\n",
        "  df_with_context = create_n_gram_df(df, n_pad=n_pad)\n",
        "\n",
        "  char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
        "             ['char-' + str(i + 1) for i in range(n_pad_2)] + ['char']\n",
        "\n",
        "  # convert pandas dataframe to numpy array to feed to the model\n",
        "  x_char = df_with_context[char_row].as_matrix()\n",
        "  y = df_with_context['target'].astype(int).as_matrix()\n",
        "\n",
        "  return x_char, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV7_tlICG5t7",
        "colab_type": "code",
        "outputId": "b7b135c6-6c36-474a-f838-f92bbdb0613d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Path to the preprocessed data\n",
        "best_processed_path = 'corpora/BEST'\n",
        "\n",
        "# Load preprocessed BEST corpus\n",
        "x_train_char, y_train = prepare_feature(best_processed_path, option='train')\n",
        "x_val_char, y_val = prepare_feature(best_processed_path, option='val')\n",
        "x_test_char, y_test = prepare_feature(best_processed_path, option='test')\n",
        "\n",
        "# As a sanity check, we print out the size of the training, val, and test data.\n",
        "print('Training data shape: ', x_train_char.shape)\n",
        "print('Training data labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', x_val_char.shape)\n",
        "print('Validation data labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', x_test_char.shape)\n",
        "print('Test data labels shape: ', y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (16461637, 21)\n",
            "Training data labels shape:  (16461637,)\n",
            "Validation data shape:  (2035694, 21)\n",
            "Validation data labels shape:  (2035694,)\n",
            "Test data shape:  (2271932, 21)\n",
            "Test data labels shape:  (2271932,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAGZtQmla9fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def get_feedforward_nn():\n",
        "  input1 = Input(shape=(21,))\n",
        "  x = Dense(100, activation='relu')(input1)\n",
        "  x = Dense(100, activation='relu')(x)\n",
        "  x = Dense(100, activation='relu')(x)\n",
        "  out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs=input1, outputs=out)\n",
        "  model.compile(optimizer=Adam(),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJA84w3zbxxQ",
        "colab_type": "code",
        "outputId": "4aef4a55-4d35-4b67-851b-d36e1803a220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "# This is called to clear the original model session in order to use TensorBoard\n",
        "#from keras import backend as K\n",
        "#K.clear_session()\n",
        "\n",
        "# Path to save model parameters\n",
        "weight_path_feedforward_nn='model_weight_feedforward_nn.h5'\n",
        "\n",
        "# Training callbacks list. TensorBoard() write logs for tensorboard GUI. \n",
        "# ModelCheckpoint() writes the resulting model.\n",
        "# Note that writing to disk takes time (longer than model training time). \n",
        "# For other sections, you might not writing any files to disk \n",
        "# or write only the graph for TensorBoard.\n",
        "callbacks_list_feedforward_nn = [\n",
        "        TensorBoard(log_dir='/data/Graph/nn_ff', histogram_freq=1, write_graph=True, write_grads=False),\n",
        "        ModelCheckpoint(\n",
        "            weight_path_feedforward_nn,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=True,\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            verbose=1\n",
        "        )\n",
        "  ]\n",
        "\n",
        "print('start training')\n",
        "verbose = 1\n",
        "model_feedforward_nn = get_feedforward_nn()\n",
        "\n",
        "## load weight \n",
        "\n",
        "model_feedforward_nn.load_weights('model_weight_token/model_weight_feedforward_nn.h5')\n",
        "train_params = [(1, 512)]\n",
        "for (epochs, batch_size) in train_params:\n",
        "  print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
        "  model_feedforward_nn.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
        "                           callbacks=callbacks_list_feedforward_nn,\n",
        "                           validation_data=(x_val_char, y_val))\n",
        "# model_feedforward_nn.save_weights('model_weight_feedforward_nn.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training\n",
            "train with 3 epochs and 512 batch size\n",
            "Train on 16461637 samples, validate on 2035694 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "16459264/16461637 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9040\n",
            "Epoch 00001: val_loss improved from inf to 0.23108, saving model to model_weight_feedforward_nn.h5\n",
            "16461637/16461637 [==============================] - 140s 8us/sample - loss: 0.2319 - acc: 0.9040 - val_loss: 0.2311 - val_acc: 0.9053\n",
            "Epoch 2/3\n",
            "16458752/16461637 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9079\n",
            "Epoch 00002: val_loss improved from 0.23108 to 0.22776, saving model to model_weight_feedforward_nn.h5\n",
            "16461637/16461637 [==============================] - 138s 8us/sample - loss: 0.2241 - acc: 0.9079 - val_loss: 0.2278 - val_acc: 0.9065\n",
            "Epoch 3/3\n",
            "16460288/16461637 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9105\n",
            "Epoch 00003: val_loss improved from 0.22776 to 0.21220, saving model to model_weight_feedforward_nn.h5\n",
            "16461637/16461637 [==============================] - 139s 8us/sample - loss: 0.2187 - acc: 0.9105 - val_loss: 0.2122 - val_acc: 0.9138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cneHmvi_cCnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "\n",
        "################################################################################\n",
        "# Write a function to evaluate your model. Your function must make prediction  #\n",
        "# using the input model and return f-score, precision, and recall of the model.#\n",
        "# You can make predictions by calling model.predict().                         #\n",
        "################################################################################\n",
        "def evaluate(x_test, y_test, model):\n",
        "  \"\"\"\n",
        "  Evaluate model on the splitted 10 percent testing set.\n",
        "  \"\"\"\n",
        "  y_pred = model.predict(x_test)\n",
        "\n",
        "  #map probability to class\n",
        "  prob_to_class = lambda p: 1 if p[0]>=0.5 else 0\n",
        "  y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
        "    \n",
        "  f1score = f1_score(y_test,y_pred)\n",
        "  precision = precision_score(y_test,y_pred)\n",
        "  recall = recall_score(y_test,y_pred)\n",
        "  return f1score, precision, recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh3QMrzZcNRE",
        "colab_type": "code",
        "outputId": "85db3194-eda4-44df-9342-f7c5a149b1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(x_test_char, y_test, model_feedforward_nn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.844583083069158, 0.8761640551649669, 0.8151995513747048)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQzLsgdhqPuV",
        "colab_type": "text"
      },
      "source": [
        "## print examaple feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrptrB2M-ozu",
        "colab_type": "code",
        "outputId": "c12ce98b-2cfe-499f-f7f9-a9d32ab75cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#print char of feature 1\n",
        "char = np.array(CHARS)\n",
        "\n",
        "#A function for displaying our features in text\n",
        "def print_features(tfeature,label,index):\n",
        "    feature = np.array(tfeature[index],dtype=int).reshape(21,1)\n",
        "    #Convert to string\n",
        "    char_list = char[feature]\n",
        "    left = ''.join(reversed(char_list[10:20].reshape(10))).replace(\" \", \"\")\n",
        "    center = ''.join(char_list[20])\n",
        "    right =  ''.join(char_list[0:10].reshape(10)).replace(\" \", \"\")\n",
        "    word = ''.join([left,' ',center,' ',right])\n",
        "    print(center + ': ' + word + \"\\tpred = \"+str(label[index]))\n",
        "\n",
        "for ind in range(0,30):\n",
        "    print_features(x_train_char,y_train,ind)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ค:  ค ณะตุลาการร\tpred = 1\n",
            "ณ: ค ณ ะตุลาการรั\tpred = 0\n",
            "ะ: คณ ะ ตุลาการรัฐ\tpred = 0\n",
            "ต: คณะ ต ุลาการรัฐธ\tpred = 0\n",
            "ุ: คณะต ุ ลาการรัฐธร\tpred = 0\n",
            "ล: คณะตุ ล าการรัฐธรร\tpred = 0\n",
            "า: คณะตุล า การรัฐธรรม\tpred = 0\n",
            "ก: คณะตุลา ก ารรัฐธรรมน\tpred = 0\n",
            "า: คณะตุลาก า รรัฐธรรมนู\tpred = 0\n",
            "ร: คณะตุลากา ร รัฐธรรมนูญ\tpred = 0\n",
            "ร: คณะตุลาการ ร ัฐธรรมนูญก\tpred = 0\n",
            "ั: ณะตุลาการร ั ฐธรรมนูญกั\tpred = 0\n",
            "ฐ: ะตุลาการรั ฐ ธรรมนูญกับ\tpred = 0\n",
            "ธ: ตุลาการรัฐ ธ รรมนูญกับค\tpred = 0\n",
            "ร: ุลาการรัฐธ ร รมนูญกับคว\tpred = 0\n",
            "ร: ลาการรัฐธร ร มนูญกับควา\tpred = 0\n",
            "ม: าการรัฐธรร ม นูญกับความ\tpred = 0\n",
            "น: การรัฐธรรม น ูญกับความเ\tpred = 0\n",
            "ู: ารรัฐธรรมน ู ญกับความเป\tpred = 0\n",
            "ญ: รรัฐธรรมนู ญ กับความเป็\tpred = 0\n",
            "ก: รัฐธรรมนูญ ก ับความเป็น\tpred = 1\n",
            "ั: ัฐธรรมนูญก ั บความเป็นอ\tpred = 0\n",
            "บ: ฐธรรมนูญกั บ ความเป็นอง\tpred = 0\n",
            "ค: ธรรมนูญกับ ค วามเป็นองค\tpred = 1\n",
            "ว: รรมนูญกับค ว ามเป็นองค์\tpred = 0\n",
            "า: รมนูญกับคว า มเป็นองค์ก\tpred = 0\n",
            "ม: มนูญกับควา ม เป็นองค์กร\tpred = 0\n",
            "เ: นูญกับความ เ ป็นองค์กรต\tpred = 1\n",
            "ป: ูญกับความเ ป ็นองค์กรตุ\tpred = 0\n",
            "็: ญกับความเป ็ นองค์กรตุล\tpred = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKIiJ3_aqV4d",
        "colab_type": "text"
      },
      "source": [
        "## example DNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qVB72153tsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO#4\n",
        "# Write a function that return feedforward model with dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def get_nn_with_dropout():\n",
        "    input1 = Input(shape=(21,))\n",
        "    x = Dense(100, activation='relu')(input1)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input1, outputs=out)\n",
        "    model.compile(optimizer=Adam(0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbzOHhQW32rN",
        "colab_type": "code",
        "outputId": "fb2df1ad-0603-4419-986f-248831d161f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print('start training dropout')\n",
        "model_nn_with_dropout = get_nn_with_dropout()\n",
        "\n",
        "## load weight \n",
        "model_nn_with_dropout.load_weights('model_weight_token/model_weight_nn_dropout.h5')\n",
        "\n",
        "verbose = 1\n",
        "train_params = [(1, 512)]\n",
        "for (epochs, batch_size) in train_params:\n",
        "    print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
        "    model_nn_with_dropout.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
        "                           validation_data=(x_val_char, y_val))\n",
        "    \n",
        "# model_feedforward_nn.save_weights('model_weight_feedforward_nn.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training\n",
            "train with 1 epochs and 512 batch size\n",
            "Train on 16461637 samples, validate on 2035694 samples\n",
            "16461637/16461637 [==============================] - 149s 9us/sample - loss: 0.3427 - acc: 0.8493 - val_loss: 0.3339 - val_acc: 0.8579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILEknJz739TE",
        "colab_type": "code",
        "outputId": "0fd3eff8-c971-40b0-8eb9-af7f8dc4394d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(x_test_char, y_test, model_nn_with_dropout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7199613295276679, 0.8189623745465752, 0.6423145274032652)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy39pHHz3_FS",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzfRybFzFasb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "# TODO#6:                                                                      #\n",
        "# Write a function that returns keras convolution nueral network model.        #\n",
        "# You can choose any normalization methods, activation function, as well as    #\n",
        "# any hyperparameter the way you want. Your goal is to predict a score         #\n",
        "# between [0,1] for each input whether it is the beginning of the word or not. #\n",
        "#                                                                              #\n",
        "# Hint: You should read keras documentation to see the list of available       #\n",
        "# layers and options you can use.                                              #\n",
        "################################################################################\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Conv1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "def get_conv1d_nn():\n",
        "    input1 = Input(shape=(21,))\n",
        "    x = Embedding(len(CHARS), 32, input_length=21)(input1)\n",
        "    x = Conv1D(100,5, activation='relu',input_shape=(21, 32))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input1, outputs=out)\n",
        "    model.compile(optimizer=Adam(0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "#from tensorflow.keras import backend as K\n",
        "#K.clear_session()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqy0Df8BgLlz",
        "colab_type": "code",
        "outputId": "00cab11f-5af6-407d-90ae-d59a09722c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        " ################################################################################\n",
        "# TODO#7:                                                                      #\n",
        "# Write code that call model.fit, or model.fit_generator if you have data      #\n",
        "# generator, to train you models. Make sure you have validation_data as an     # \n",
        "# argument and use verbose=2 to generate one log line per epoch. Select your   #\n",
        "# batch size carefully as it will affect your model's ability to converge and  #\n",
        "# time needed for one epoch.                                                   #\n",
        "################################################################################\n",
        "print('start training conv1d')\n",
        "model_conv1d_nn = get_conv1d_nn()\n",
        "\n",
        "## load weight \n",
        "model_conv1d_nn.load_weights('model_weight_token/model_weight_conv_nn.h5')\n",
        "\n",
        "verbose = 1\n",
        "train_params = [(1, 512)]\n",
        "for (epochs, batch_size) in train_params:\n",
        "    print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
        "    model_conv1d_nn.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
        "                           validation_data=(x_val_char, y_val))\n",
        "\n",
        "# model_conv1d_nn.save_weights('model_weight_conv_nn.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training conv1d\n",
            "train with 1 epochs and 512 batch size\n",
            "Train on 16461637 samples, validate on 2035694 samples\n",
            "16461637/16461637 [==============================] - 316s 19us/sample - loss: 0.0393 - acc: 0.9862 - val_loss: 0.0476 - val_acc: 0.9836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6YM6dAa4Z-m",
        "colab_type": "code",
        "outputId": "7b083b99-9305-480f-b724-53ee221684e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(x_test_char, y_test, model_conv1d_nn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9744274543888461, 0.9635448519554971, 0.9855586885608448)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWICXg364git",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "################################################################################\n",
        "# TODO#8                                                                       #\n",
        "# Write a function that returns keras GRU network moded. You can choose any    #\n",
        "# normalization methods, activation function, as well as any hyperparameter    #\n",
        "# the way you want. Your goal is to predict a score between [0,1] for each     #\n",
        "# input whether it is the beginning of the word or not.                        #\n",
        "#                                                                              #\n",
        "# Hint: You should read keras documentation to see the list of available       #\n",
        "# layers and options you can use.                                              #\n",
        "################################################################################\n",
        "\n",
        "def get_gru():\n",
        "    input1 = Input(shape=(21,))\n",
        "    x = Embedding(len(CHARS), 32, input_length=21)(input1)\n",
        "    x = Bidirectional(GRU(32,return_sequences = True))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = Model(inputs=input1, outputs=out)\n",
        "    model.compile(optimizer=Adam(0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqumbhuf4i7M",
        "colab_type": "code",
        "outputId": "eca078b8-5535-4787-db44-7a8a34906725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_train_char.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16461637, 21)\n",
            "(16461637,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CINwBg44lgb",
        "colab_type": "code",
        "outputId": "74b8ddcc-abf7-4a6b-df49-d2b22307e6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "################################################################################\n",
        "# TODO#9                                                                       #\n",
        "# Write code that call model.fit, or model.fit_generator if you have data      #\n",
        "# generator, to train you models. Make sure you have validation_data as an     # \n",
        "# argument and use verbose=2 to generate one log line per epoch. Select your   #\n",
        "# batch size carefully as it will affect your model's ability to converge and  #\n",
        "# time needed for one epoch.                                                   #\n",
        "################################################################################\n",
        "\n",
        "print('start training gru')\n",
        "model_gru = get_gru()\n",
        "\n",
        "## load weight \n",
        "model_gru.load_weights('model_weight_token/model_weight_gru.h5')\n",
        "\n",
        "verbose = 1\n",
        "train_params = [(1, 512)]\n",
        "for (epochs, batch_size) in train_params:\n",
        "    print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
        "    model_gru.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
        "                           validation_data=(x_val_char, y_val))\n",
        "    \n",
        "# model_gru.save_weights('model_weight_gru.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training gru\n",
            "train with 1 epochs and 512 batch size\n",
            "Train on 16461637 samples, validate on 2035694 samples\n",
            "16461637/16461637 [==============================] - 539s 33us/sample - loss: 0.0314 - acc: 0.9892 - val_loss: 0.0403 - val_acc: 0.9865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9XDsY0L4nn_",
        "colab_type": "code",
        "outputId": "647b8809-609a-4ab2-d084-7246329ef2a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(x_test_char, y_test, model_gru)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9782675869375349, 0.9763266500620705, 0.9802162563482849)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y_RIQK3cWrp",
        "colab_type": "code",
        "outputId": "a50e12c7-7bcd-4c39-bea4-c621a31d5532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpora  __MACOSX\t\t\t model_weight_token  sample_data\n",
            "gdrive\t model_weight_feedforward_nn.h5  new_word_th.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ZHGhYicUyM",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmTJnfzokwSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}